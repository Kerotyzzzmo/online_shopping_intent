---
title: "Discriminant Analysis"


```{r}
rm(list=ls())
data <- read.csv('online_shoppers_intention.csv', header = TRUE)
```

```{R}
library(dplyr)
data <- data %>% mutate(Month = case_when(Month == "Feb" ~ 2,
                                     Month == "Mar" ~ 3,
                                     Month == "May" ~ 5,
                                     Month == "June" ~ 6,
                                     Month == "Jul" ~ 7,
                                     Month == "Aug" ~ 8,
                                     Month == "Sep" ~ 9,
                                     Month == "Oct" ~ 10,
                                     Month == "Nov" ~ 11,
                                     Month == "Dec" ~ 12))

data <- data %>% mutate(VisitorType = case_when(VisitorType == "Returning_Visitor" ~ 1, 
                                     VisitorType == "New_Visitor" ~ 2,
                                     VisitorType == "Other" ~ 3))

data <- data %>% mutate(Weekend = case_when(Weekend == TRUE ~ 1, 
                                     Weekend == FALSE ~ 0))

data <- data %>% mutate(Revenue = case_when(Revenue == TRUE ~ 1, 
                                       Revenue == FALSE ~ 0))

data$OperatingSystems = as.integer(data$OperatingSystems)
data$Browser = as.integer(data$Browser)
data$Region = as.integer(data$Region)
data$TrafficType = as.integer(data$TrafficType)
```


```{R}
library(caret)

set.seed(7406)
index <- createDataPartition(data$Revenue, p=0.80, list = FALSE, times = 1)
df_train = data[index,]
df_test = data[-index,]

table(df_test$Revenue)
prop.table(table(df_test$Revenue))

table(df_train$Revenue)
prop.table(table(df_train$Revenue))

```

As we can see, the original dataset contains 15.47% of TRUE cases and 85.43% FALSE cases. We have imbalanced data set. One way to handle imbalanced data set is oversampling, which works with minority class. An advantage of using this method is that it leads to no information loss. The disadvantage of using this method is that, since oversampling simply adds replicated observations in original data set, it ends up adding multiple observations of several types, thus leading to overfitting. Although, the training accuracy of such data set will be high, but the accuracy on unseen data will be worse.

USE package 'ROSE' to handle imbalanced data. 

```{r}
library(ROSE)

df_rose <- ROSE(Revenue~., data = df_train, seed =1)$data
table(df_rose$Revenue)
```

```{r}
library(MASS)
m0_lda <- lda(df_rose[,1:17], df_rose[,18])
m0_lda
```

The aim of LDA:
-maximize the distance between two groups (the classes' group mean is placed as far as possible to ensure high confidence during prediction) 

The LDA output indicates that our prior probability is 50.39% for group 0(Revenue False) and 49.61% for group 1(Revenue TRUE). 


```{r}
m0_lda$means
```

We can use stacked histogram to display discriminant function values from different groups. We can see the separation between these two groups is quite close with lots of overlapping.


```{r}
lda.values <- predict(m0_lda)
ldahist(data = lda.values$x[,1], g=df_rose$Revenue)
```


```{r}
library(caret)
df_test$Revenue<-as.factor(df_test$Revenue)
pred_lda <- predict(m0_lda,df_test[,1:17])$class
ConMatrix_lda <- confusionMatrix(data = pred_lda, reference = df_test$Revenue)
ConMatrix_lda
```

```{r}
#fit another model with variables that have significant differences in group mean
df_rose1 <- df_rose[,c(2,4,5,6,9,18)]
m1_lda <- lda(df_rose1[,1:5], df_rose1[,6])
m1_lda
```

```{r}
library(klaR)
df_rose1$Revenue <- as.factor(df_rose1$Revenue)
partimat(Revenue~.,data=df_rose1,method="lda")
```

As we can see, there are lots of overlapping between two groups. 

```{r}
plot(m1_lda)
```

```{r}
library(caret)
df_test$Revenue<-as.factor(df_test$Revenue)
df_test1 <- df_test[,c(2,4,5,6,9,18)]
pred1_lda <- predict(m1_lda,df_test1[,1:5])$class

ConMatrix_lda1 <- confusionMatrix(data = pred1_lda, reference = df_test1$Revenue)
ConMatrix_lda1
```

With fewer variables, we are able to improve model accuracy (misclassification rate decreased) but we have more cases of false positives (our model predicted the customers will make purchase but they don't actually make the purchase). 


```{r}
m_qda <- qda(df_rose[,1:17], df_rose[,18])
m_qda
```


```{r}
pred_qda <- predict(m_qda,df_test[,1:17])$class
ConMatrix_qda <- confusionMatrix(data = pred_qda, reference = df_test$Revenue)
ConMatrix_qda
```


```{r}
library(ROSE)
B=100
set.seed(7406)
TEALL = NULL 
for (b in 1:B){
  index_cv <- createDataPartition(data$Revenue, p=0.80, list = FALSE, times = 1)
  df_train_cv = data[index_cv,]
  df_test_cv = data[-index_cv,]
  df_rose_cv <- ROSE(Revenue~., data = df_train_cv, seed =1)$data
  
  m1_lda <-lda(df_rose_cv[,1:17],df_rose_cv[,18])
  pred1_cv <- predict(m1_lda, df_test_cv[,1:17])$class
  TE_lda <- mean(pred1_cv == df_test_cv$Revenue)
  
  m1_qda <-qda(df_rose_cv[,1:17],df_rose_cv[,18])
  pred2_cv <- predict(m1_qda, df_test_cv[,1:17])$class
  TE_qda <- mean(pred2_cv == df_test_cv$Revenue)
  
  TEALL <- rbind(TEALL,cbind(TE_lda,TE_qda))
} 


colnames(TEALL) <- c("LDA", "QDA")              
round(apply(TEALL, 2, mean),5)
round(apply(TEALL, 2, var),5)
```
Overall, LDA performs better than QDA, since LDA have higher model accuracy. 


```{r}
coef = c(2.337417e-02,1.320386e-04,1.467700e-02,5.623113e-04,1.926169e-03,8.007664e-05,-1.556938e+00,-7.522570e+00,2.496501e-02,-5.601350e-01,5.360545e-02,-4.187995e-02,7.485834e-03,-1.361906e-02,-1.098067e-03,1.579287e-01,1.375565e-01)
var = colnames(df_rose[,1:17])

df_coef <- as.data.frame(cbind(var,as.numeric(coef)))
ggplot(df_coef,aes(x=var,y=coef, fill=var))+geom_bar(stat="identity")+ coord_flip()+ theme(legend.position="none")+xlab('Variable')+ylab('LDA Coefficient')
```


